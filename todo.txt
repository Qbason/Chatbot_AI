TODOS:
    - create docker-compose.yaml for local deployment
    - implement backend:
        - create mockup for OPENAI service -> lorem ipsum etc.
        - implement if there will be enough time to return message_id and conversation_id in stream api
        - optimize saving chunks - too much IO database
    - implement frontend
        - implement resume <- handle stream_status to decide
        - improve scrolling
        - improve warning while logging in (bad credentials)
        - add markdown to message
    - write readme how to run


Thoughts:
    Idea of api:
        /api/conversations for getting all conversations
        /api/conversations/{id} for getting details about
        /api/conversations POST it creates conversation and stream via EventStream
        /api/conversations/{id}/stream_status if is streaming -> resume
        /api/conversations/message_feedback -> conversation_id, message_id, rating: "thumbsUp/thumbsDown"

        /api/chat/resume/stream -stream content of AI message -> event delta for chunks
        /api/chat/resume used when conversation is still going it streams output
        /api/chat/stop_conversation -> stop streaming (cancelation token) by conversation id
